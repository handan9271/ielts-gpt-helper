from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import openai
import os
import json

client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ğŸ‘ˆ å»ºè®®ä½ ä¸´æ—¶å¼€æ”¾æ‰€æœ‰åŸŸåæµ‹è¯•ï¼ˆå¯åç»­é™åˆ¶ï¼‰
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class IELTSRequest(BaseModel):
    question: str
    input: str

PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½é¢å‘é›…æ€6åˆ†ä»¥ä¸‹è€ƒç”Ÿçš„ AI å†™ä½œä¸å£è¯­åŠ©æ•™ï¼Œå¸®åŠ©å­¦ç”Ÿå°†ä¸­æ–‡æ€è·¯è½¬åŒ–ä¸ºç»“æ„æ¸…æ™°ã€è¯­è¨€ä¸°å¯Œã€é€æ­¥æå‡çš„è‹±æ–‡è¡¨è¾¾ï¼Œå¹¶å¼•å¯¼ä»–ä»¬æŒæ¡æåˆ†å…³é”®è¡¨è¾¾ã€‚ä½ çš„ä»»åŠ¡åŒ…æ‹¬ä»¥ä¸‹äº”æ­¥ï¼š

---

ã€TEELç»“æ„è‹±æ–‡è¡¨è¾¾ï¼ˆé€å¥è¾“å‡º + ä¸­æ–‡ç¿»è¯‘ + é«˜äº®è¡¨è¾¾ï¼‰ã€‘
å­¦ç”Ÿä¸­æ–‡æ€è·¯å¦‚ä¸‹ï¼š{input_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é€æ­¥ç”Ÿæˆè‹±æ–‡å›ç­”ï¼Œæ¯å¥è‹±æ–‡ä¸‹æ–¹é™„ä¸­æ–‡ç¿»è¯‘ï¼Œé‡è¦è¡¨è¾¾ç”¨**åŠ ç²—**æˆ–==é«˜äº®==è¡¨ç¤ºã€‚æ¯æ®µä¸è¶…è¿‡6è¡Œã€‚

ã€è¯æ±‡è®²è§£ã€‘
æŒ‘é€‰å½“å‰æ®µè½ä¸­å­¦ç”Ÿå¯èƒ½ä¸ç†Ÿæ‚‰çš„è¯æ±‡ï¼ˆ3-5ä¸ªï¼‰ï¼Œå†™å‡ºè‹±æ–‡ã€ä¸­æ–‡è§£é‡Šã€å¹¶ç»™å‡ºä¾‹å¥ã€‚

ã€AIè¯„åˆ† + ä¸­æ–‡åé¦ˆå»ºè®®ã€‘
æ ¹æ® IELTS å†™ä½œè¯„åˆ†æ ‡å‡†ï¼Œä¸ºå­¦ç”Ÿä¼°ç®—ä¸€ä¸ª Band åˆ†ï¼Œå¹¶æŒ‡å‡ºä¼˜ç‚¹å’Œæ”¹è¿›å»ºè®®ã€‚

ã€é«˜åˆ†å‚è€ƒæ®µè½ï¼ˆè‹±æ–‡ + ä¸­æ–‡ + é«˜äº®è¡¨è¾¾ï¼‰ã€‘
ç”Ÿæˆä¸€ä¸ªæ›´é«˜åˆ†æ•°ï¼ˆ+1åˆ†ï¼‰çš„å®Œæ•´è‹±æ–‡æ®µè½ï¼Œä¸å­¦ç”ŸåŸå§‹è¡¨è¾¾ç›¸åŒä¸»é¢˜ï¼Œé€å¥ç¿»è¯‘å¹¶é«˜äº®è¡¨è¾¾ã€‚
"""

@app.post("/api/ielts-helper")
async def ielts_helper(data: IELTSRequest):
    prompt = PROMPT_TEMPLATE.format(input_text=data.input, question=data.question)

    def gen():
        response = client.chat.completions.create(
            model="gpt-4-0125-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé›…æ€AIåŠ©æ•™"},
                {"role": "user", "content": prompt}
            ],
            stream=True,
        )
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.get("content"):
                yield chunk.choices[0].delta["content"]

    return StreamingResponse(gen(), media_type="text/plain")

@app.get("/api/random-question")
async def random_question():
    prompt = """
ä½ æ˜¯ä¸€ä½é›…æ€å£è¯­è€ƒå®˜åŠ©ç†ï¼Œè¯·ä½ ä»æœ€è¿‘çœŸå®çš„ Part 2 é¢˜åº“ä¸­ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªçƒ­é—¨ä¸”å…·å¯å‘æ€§çš„é¢˜ç›®ï¼Œè¿”å› JSON æ ¼å¼ï¼š

è‹±æ–‡é¢˜ç›®å­—æ®µæ˜¯ "en"ï¼Œä¸­æ–‡ç¿»è¯‘å­—æ®µæ˜¯ "zh"ã€‚

ç¤ºä¾‹ï¼š
{
  "en": "Describe a time you received positive feedback.",
  "zh": "æè¿°ä¸€æ¬¡ä½ æ”¶åˆ°ç§¯æåé¦ˆçš„ç»å†"
}
åªè¾“å‡º JSONï¼Œå…¶ä»–å†…å®¹ä¸è¦è¾“å‡ºã€‚
"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé›…æ€å£è¯­é¢˜åº“ç”Ÿæˆå™¨"},
            {"role": "user", "content": prompt.strip()}
        ]
    )
    try:
        return json.loads(response.choices[0].message.content)
    except:
        return {
            "en": "Describe a piece of technology you use frequently.",
            "zh": "æè¿°ä¸€ä¸ªä½ ç»å¸¸ä½¿ç”¨çš„ç§‘æŠ€äº§å“ï¼ˆå¤‡ç”¨é¢˜ï¼‰"
        }
